{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import tweepy\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import and Initialize Sentiment Analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# Twitter API Keys\n",
    "from config import twt_cmr_ak, twt_cmr_ak_s, twt_acc_tkn, twt_acc_tkn_s\n",
    "\n",
    "\n",
    "# Setup Tweepy API Authentication\n",
    "auth = tweepy.OAuthHandler(twt_cmr_ak, twt_cmr_ak_s)\n",
    "auth.set_access_token(twt_acc_tkn, twt_acc_tkn_s)\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "my_posts = ['1049024330636509184'] #This is my last tweet id before trying to deploy to Heroku\n",
    "\n",
    "past_requests = {}  #This dictionary will later store keys as twitter handles for who has been analyzed, with values of the twitter ID where the analysis was posted.\n",
    "#e.g. '@paizo' : '1049011716934098945'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''PullTweets gets 500 tweets from target, returns them in a list'''\n",
    "def PullTweets(target):  #Target should be a string that is a twitter user name\n",
    "    x=1 #iteration counter - decided to use while instead of for loop because I thought it made exception handling easier\n",
    "    tw_get = [] #blank list, will get filled with tweet json dictionaries\n",
    "    while x < 6: #goes through 5 pages\n",
    "        try:\n",
    "            raw_get = api.user_timeline(target, page=x, count=100) #Gets a page of 100 tweets\n",
    "            tw_get += raw_get #appends them to the existing tweets - has its own variable so we can count how many we got back from this page\n",
    "            print('got page ' + str(x)) #just some flow control to keep track of what page has been pulled successfully\n",
    "            if len(raw_get) <100 :#If the page had less than the max number of tweets, you can stop trying to pull more - you'll get empty results at best.\n",
    "                print('ran out of tweets')\n",
    "                break \n",
    "            x+=1 #iterate our loop\n",
    "            \n",
    "        except tweepy.RateLimitError: #if we hit rate limit, take a 15 minute break - then continue on the while loop.\n",
    "            time.sleep(15 * 60) #Could have written 900 seconds, but I saw 15 * 60 in the Tweepy documentation and thought it was clearer\n",
    "            print('Rate Limit Error')\n",
    "    print('Pulled ' + str(len(tw_get)) + ' tweets.' )\n",
    "    return tw_get #Sends the 0-500 tweets back in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Runs a VADER analysis on passed tweet data, returns analysis as a dictionary'''\n",
    "def AnalyzeTweets(data):\n",
    "    output = {\n",
    "        'com' : [],\n",
    "        'id' : []}\n",
    "    #In the past we've used 4 lists - one for each return by the VADER politary score function.  Our graph only uses one of these metrics.\n",
    "    for tweet in data: \n",
    "        v_analysis = analyzer.polarity_scores(tweet['text'])\n",
    "        output['com'].append(v_analysis['compound'])\n",
    "        output['id'].append(tweet['id'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sort_Data will sort the passed VADER Dictionary into a dataframe and return a Dataframe sorted by ID'''\n",
    "def SortData(sent):\n",
    "    output_pd = pd.DataFrame({\n",
    "        'Tweet ID' : sent['id'],\n",
    "        'Compound' : sent['com'],\n",
    "    })\n",
    "    output_pd = output_pd.sort_values('Tweet ID') \n",
    "    #IDs are assigned chronologically, so sorting from lowest to highest lets us plot from earlier to later\n",
    "    #We're only required to sort tweets relative to eachother, we don't need the date.\n",
    "    return output_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Graph_Tweets takes the passed PD and returns a matplotlib figure of the analysis'''\n",
    "def GraphTweets(df, target, figtitle):\n",
    "    date=datetime.datetime.now().strftime('%m/%d, %Y')\n",
    "    fig = plt.figure()\n",
    "    x_axis = np.arange(len(df)*-1, 0)\n",
    "    #Presumably this will usually return 0 to 500, but we'll use arange just in case we got fewer than 500 tweets back.\n",
    "    plt.plot(x_axis, df['Compound'], color='slateblue', markersize=4, linewidth=0.5, marker = 'o')\n",
    "    plt.xlabel('Tweets Ago')\n",
    "    plt.ylabel('Tweet Polarity')\n",
    "    plt.ylim(-1.1, 1.1)\n",
    "    plt.xlim(-510, 10)\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.title(f'Sentiment Analysis of {target} on {date}')\n",
    "    fig.savefig(figtitle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Check_Feed checks my status since the last requests tweet ID and returns all tweets in one JSON'''\n",
    "def CheckFeed():\n",
    "    try:\n",
    "        recent_tweets = api.mentions_timeline(since_id = my_posts[-1])\n",
    "    except tweepy.RateLimitError:  #Recurses the function 15 minutes from now if I hit the rate limit\n",
    "        time.sleep(15*60)\n",
    "        Check_Feed()\n",
    "    return recent_tweets   #Not sure how this performs with a huge tweet volume within 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''GetReqTag takes a tweet and returns the handle to analyze'''\n",
    "def GetTargetTag(tweet):\n",
    "    tag = '@' + tweet['entities']['user_mentions'][1]['screen_name'] \n",
    "                #Usually they want you to analyze the second mention, while the first (index 0) mention will be me.\n",
    "        #add the tag to the list of things to examine\n",
    "    return tag\n",
    "\n",
    "'''GetCustTag takes a tweet and returns the handle of the person asking for the analysis'''\n",
    "def GetCustTag(tweet):\n",
    "    tag = '@' + tweet['entities']['user_mentions'][0]['screen_name']\n",
    "    return tag\n",
    "def GetReqID(tweet):\n",
    "    reqID = tweet['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Sends the tweet with the graphic on it'''\n",
    "def ReplyTweet(graph_path, target, customer, sentiment, request_id):\n",
    "    tweettext = f'Analysis of {target}, as requested by {customer}.  Mean Compound Sentiment: {sentiment}'\n",
    "    try:\n",
    "        out = api.update_with_media(graph_path, tweettext, request_id)\n",
    "        return str(out['id'])\n",
    "    except tweepy.RateLimitError:  #Recurses the function 15 minutes from now if I hit the rate limit\n",
    "        time.sleep(15*60)\n",
    "        ReplyTweet(graph_path,target,customer,sentiment,request_id)\n",
    "\n",
    "'''Replies when someone asks for a duplicate analysis.'''\n",
    "def DirectToPrevious(customer_handle, target, request_id):\n",
    "    previous_id = str(past_requests[target])\n",
    "    tweettext = f'Hi, {customer_handle}, this as an old one:    https://twitter.com/DustinGR2/status/{previous_id}'\n",
    "    try:\n",
    "        api.update_status(tweettext, request_id)\n",
    "    except tweepy.RateLimitError:  #Recurses the function 15 minutes from now if I hit the rate limit\n",
    "        time.sleep(15*60)\n",
    "        DirectToPrevious(customer_handle,target,request_id)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Coordinates other functions''' \n",
    "def HandleTweets(tweets):\n",
    "    for tweet in tweets:\n",
    "        request_id = tweet['id_str']  #Gets the tweet ID for this request\n",
    "        customer_handle = '@' + tweet['user']['screen_name'] #Find the handle of the person tweeting at me\n",
    "        if len(tweet['entities']['user_mentions']) > 1 and tweet['text'].lower().find('analyze') >=0: #This will leave the tweet alone if there is not more than one mention in it\n",
    "            target = '@' + tweet['entities']['user_mentions'][1]['screen_name'] #gets the id of the target account - the second mention\n",
    "            if target in past_requests:   #Check if this request has been made before\n",
    "                DirectToPrevious(customer_handle, target, request_id)   #This function will tell the requesting person where the earlier graph is\n",
    "            else:\n",
    "                try:\n",
    "                    tweets_to_analyze = PullTweets(target)    #Downloads tweets\n",
    "                    analysis_df = SortData(AnalyzeTweets(tweets_to_analyze)) #does vader analysis and returns a dataframe\n",
    "                    figtitle = datetime.datetime.now().strftime('%B_%Y') + target[1:] + '.png' #Builds a filename for the graph\n",
    "                    GraphTweets(analysis_df, target, figtitle) #Creates the graph with the filename figtitle\n",
    "                    sentiment = round(analysis_df['Compound'].mean(), 5) #Gets the mean sentiment to mention in the reply tweet\n",
    "                    reply_id = ReplyTweet(figtitle, target, customer_handle, sentiment, request_id) # makes the reply tweet, and stores the tweet ID\n",
    "                    past_requests[target] = reply_id   #This adds the reply's ID to the list of requests so it can be linked to people who request a repeat\n",
    "                except:\n",
    "                    print(sys.exc_info()[0]) #prints errors \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0d23a669f7f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mHandleTweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m#HandleTweets will check for analysis tweets and make necessary replies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Wait 5 minutes and do it again!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    check = CheckFeed() \n",
    "    #Check status for new tweets\n",
    "    if len(check) > 0:\n",
    "        #If there are new tweets...\n",
    "        my_posts.append(check[-1]['id_str'])   #appends the last tweet id to the end of the requests list, which is used on the Check_Feed\n",
    "        HandleTweets(check)\n",
    "        #HandleTweets will check for analysis tweets and make necessary replies\n",
    "    time.sleep(5*60) #Wait 5 minutes and do it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
